# AI_Disclosure_Trust_Experiment_Stimuli
This repository contains the complete set of experimental stimuli used in the 2×2 between-subjects design of my master's thesis in Communication Science at the University of Amsterdam. The study investigates how **source type** (institutional vs. personal) and **AI authorship disclosure** (present vs. absent) influence **perceived trust** and **message credibility** in the context of European Union digital rights communication.

## Repository Structure

Each folder represents one of the four experimental conditions and contains a fully rendered HTML page replicating a social media profile and post on X (formerly Twitter):

- `condition1-main/`: Institutional source *without* AI authorship label  
- `condition2-2-main/`: Institutional source *with* AI authorship label  
- `condition3-main/`: Personal source *without* AI authorship label  
- `condition4-main/`: Personal source *with* AI authorship label  

### Contents of Each Folder

Each folder contains:
- `index.html`: A self-contained mock-up of a public-facing **X profile and post**, created using [Lovable.dev](https://lovable.dev). 
- Embedded profile features including display name, handle, bio, follower/following counts, profile photo, verified badge (when applicable), post content, and timestamp.
- Disclosure labels (when shown) are integrated directly into the post text in a format resembling real-world AI usage notices.

All files are fully functional HTML pages that can be opened in any browser and require no external dependencies.

## Purpose and Theoretical Context

These materials were designed to test the psychological impact of two key digital heuristics—**source identity** and **AI disclosure**—on users’ evaluation of credibility and institutional trustworthiness in online settings. The design draws on the **Heuristic-Systematic Model (HSM)** and the **Modality–Agency–Interactivity–Navigability (MAIN) Model**, which explain how users form judgments based on surface-level cues in digital environments.

## Reuse and Citation

These materials are shared to support transparency, reproducibility, and future academic research in areas such as:
- Political and institutional communication
- Digital media trust and misinformation studies
- Online experimental design
- Media literacy education

If you reuse or adapt these stimuli, please cite this repository as:

> Skalli, Y. (2025). *AI Trust Experiment Stimuli: Simulated X profiles for EU digital rights communication study* [Data set]. GitHub. https://github.com/yourusername/ai-trust-experiment-stimuli

For questions or permissions beyond academic reuse, please contact the author directly.
